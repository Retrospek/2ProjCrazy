{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Simple Model Approach***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(\"trainData2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(embedding_model='word2vec'):\n",
    "    \"\"\"Load the appropriate embedding model.\"\"\"\n",
    "    return api.load('word2vec-google-news-300')  # This is a large model, can be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_embedding_model(embedding_model='word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample CSV reading (assuming the CSV is properly formatted)\n",
    "df = pd.read_csv(\"trainData2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedConnections:\n",
    "    def __init__(self, num_clusters=5):\n",
    "        \"\"\"\n",
    "        Initializes the UnsupervisedConnections class with clustering configurations.\n",
    "        \"\"\"\n",
    "        self.num_clusters = num_clusters\n",
    "    \n",
    "    def agglomerative_clustering(self, embeddings):\n",
    "        \"\"\"Perform Agglomerative clustering on the word embeddings.\"\"\"\n",
    "        agglomerative = AgglomerativeClustering(n_clusters=self.num_clusters)\n",
    "        clusters = agglomerative.fit_predict(embeddings)  # Perform clustering\n",
    "        return clusters\n",
    "\n",
    "    def kmeans_clustering(self, embeddings):\n",
    "        \"\"\"Perform K-means clustering on the word embeddings.\"\"\"\n",
    "        kmeans = KMeans(n_clusters=self.num_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)  # Perform clustering\n",
    "        return clusters\n",
    "\n",
    "    def dbscan_clustering(self, embeddings, eps=0.5, min_samples=5):\n",
    "        \"\"\"Perform DBSCAN clustering on the word embeddings.\"\"\"\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        clusters = dbscan.fit_predict(embeddings)  # Perform clustering\n",
    "        return clusters\n",
    "\n",
    "    def gmm_clustering(self, embeddings):\n",
    "        \"\"\"Perform GMM clustering on the word embeddings.\"\"\"\n",
    "        gmm = GaussianMixture(n_components=self.num_clusters, random_state=42)\n",
    "        clusters = gmm.fit_predict(embeddings)  # Perform clustering\n",
    "        return clusters\n",
    "\n",
    "    def majority_voting(self, labels_list):\n",
    "        \"\"\"\n",
    "        Aggregate the labels from different clustering algorithms using majority voting.\n",
    "        \"\"\"\n",
    "        labels_arr = np.array(labels_list).T  # Convert to a matrix of labels\n",
    "        ensemble_labels = []\n",
    "        for row in labels_arr:\n",
    "            # Exclude -1 from DBSCAN (consider it as noise)\n",
    "            row = row[row != -1]\n",
    "            # Get the most frequent label (mode) in the row (if there's a tie, pick the first)\n",
    "            if len(row) > 0:\n",
    "                most_common = np.bincount(row).argmax()  # Most common cluster\n",
    "            else:\n",
    "                most_common = -1  # In case all labels are -1 (noise points)\n",
    "            ensemble_labels.append(most_common)\n",
    "        return np.array(ensemble_labels)\n",
    "\n",
    "    def get_word_embeddings(self, model, words):\n",
    "        \"\"\"Get word embeddings using the chosen model.\"\"\"\n",
    "        embeddings = []\n",
    "        for word in words:\n",
    "            if word in model.key_to_index:  # Check if word is in the vocabulary\n",
    "                embeddings.append(model[word])\n",
    "            else:\n",
    "                embeddings.append(np.zeros(model.vector_size))  # Return a zero vector for unknown words\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def cluster_words(self, model, sample_words_data):\n",
    "        \"\"\"Cluster the words using multiple clustering algorithms and ensemble their results.\"\"\"\n",
    "        final_results = {}\n",
    "\n",
    "        # Process each problem (list of 5 clusters) separately\n",
    "        for problem_index, problem in enumerate(sample_words_data):\n",
    "            print(f\"Clustering Problem {problem_index + 1}\")\n",
    "\n",
    "            # Flatten all words from the current problem into a single list\n",
    "            all_words = [word for cluster in problem for word in cluster]\n",
    "\n",
    "            # Extract word embeddings for the flattened list of words\n",
    "            embeddings = self.get_word_embeddings(model, all_words)\n",
    "\n",
    "            # Apply all clustering models\n",
    "            agglomerative_clusters = self.agglomerative_clustering(embeddings)\n",
    "            kmeans_clusters = self.kmeans_clustering(embeddings)\n",
    "            dbscan_clusters = self.dbscan_clustering(embeddings)\n",
    "            gmm_clusters = self.gmm_clustering(embeddings)\n",
    "\n",
    "            # Aggregate the results using majority voting\n",
    "            combined_clusters = self.majority_voting([agglomerative_clusters, kmeans_clusters, dbscan_clusters, gmm_clusters])\n",
    "\n",
    "            # Assign clusters to words\n",
    "            word_cluster_mapping = {}\n",
    "            idx = 0\n",
    "            for cluster in problem:\n",
    "                for word in cluster:\n",
    "                    word_cluster_mapping[word] = combined_clusters[idx]\n",
    "                    idx += 1\n",
    "\n",
    "            # Group words by cluster\n",
    "            clustered_words = {i: [] for i in range(self.num_clusters)}\n",
    "            for word, cluster in word_cluster_mapping.items():\n",
    "                clustered_words[cluster].append(word)\n",
    "\n",
    "            # Ensure each cluster contains exactly 4 words, redistributing if necessary\n",
    "            adjusted_clusters = {i: [] for i in range(self.num_clusters)}\n",
    "            excess_words = []  # To store words that exceed the limit of 4 per cluster\n",
    "\n",
    "            # Process clusters with more than 4 words\n",
    "            for cluster, words in clustered_words.items():\n",
    "                if len(words) > 4:\n",
    "                    # If the cluster exceeds 4 words, redistribute excess words\n",
    "                    excess_words.extend(words[4:])\n",
    "                    adjusted_clusters[cluster] = words[:4]\n",
    "                else:\n",
    "                    adjusted_clusters[cluster] = words\n",
    "\n",
    "            # Redistribute the excess words to the clusters that have less than 4 words\n",
    "            excess_idx = 0\n",
    "            for cluster, words in adjusted_clusters.items():\n",
    "                while len(words) < 4 and excess_idx < len(excess_words):\n",
    "                    words.append(excess_words[excess_idx])\n",
    "                    excess_idx += 1\n",
    "\n",
    "            # Store the result in the final_results dictionary\n",
    "            final_results[f\"Problem {problem_index + 1}\"] = adjusted_clusters\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample CSV reading (assuming the CSV is properly formatted)\n",
    "df = pd.read_csv(\"trainData2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UnsupervisedConnections' object has no attribute 'train_word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize the UnsupervisedConnections model and train it\u001b[39;00m\n\u001b[0;32m     18\u001b[0m unsupervised_model \u001b[38;5;241m=\u001b[39m UnsupervisedConnections(num_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43munsupervised_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_word2vec\u001b[49m(sample_words_data)  \u001b[38;5;66;03m# Train Word2Vec on the provided words\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Cluster the words and get the embeddings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m all_clustered_words, all_embeddings \u001b[38;5;241m=\u001b[39m unsupervised_model\u001b[38;5;241m.\u001b[39mcluster_words(sample_words_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'UnsupervisedConnections' object has no attribute 'train_word2vec'"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to hold lists of words for each Game ID\n",
    "games_dict = {}\n",
    "\n",
    "# Loop through each row and add words to the respective Game ID list\n",
    "for _, row in df.iterrows():\n",
    "    game_id = row['Game ID']\n",
    "    words = [row['word1'], row['word2'], row['word3'], row['word4']]\n",
    "    \n",
    "    if game_id not in games_dict:\n",
    "        games_dict[game_id] = []\n",
    "    \n",
    "    games_dict[game_id].extend(words)\n",
    "\n",
    "# Convert the dictionary to a list of lists\n",
    "sample_words_data = list(games_dict.values())\n",
    "\n",
    "# Initialize the UnsupervisedConnections model and train it\n",
    "unsupervised_model = UnsupervisedConnections(num_clusters=4)\n",
    "unsupervised_model.train_word2vec(sample_words_data)  # Train Word2Vec on the provided words\n",
    "\n",
    "# Cluster the words and get the embeddings\n",
    "all_clustered_words, all_embeddings = unsupervised_model.cluster_words(sample_words_data)\n",
    "\n",
    "# Print the clustered words for each game\n",
    "for i, game_clusters in enumerate(all_clustered_words):\n",
    "    print(f\"Game {i+1}:\")\n",
    "    for cluster, words in game_clusters.items():\n",
    "        print(f\"  Cluster {cluster}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Problem 1\n",
      "Clustering Problem 2\n",
      "Clustering Problem 3\n",
      "Clustering Problem 4\n",
      "\n",
      "Problem 1:\n",
      "  Cluster 0: ['computer', 'laptop', 'tablet', 'smartphone']\n",
      "  Cluster 1: ['dog', 'cat', 'rabbit', 'elephant']\n",
      "  Cluster 2: ['table', 'chair', 'sofa', 'couch']\n",
      "  Cluster 3: ['apple', 'banana', 'fruit', 'grape']\n",
      "  Cluster 4: ['car', 'truck', 'bus', 'motorcycle']\n",
      "\n",
      "Problem 2:\n",
      "  Cluster 0: ['actor', 'producer', 'screenwriter', 'developer']\n",
      "  Cluster 1: ['professor', 'director', 'manager', 'designer']\n",
      "  Cluster 2: ['chef', 'waiter', 'bartender', 'cook']\n",
      "  Cluster 3: ['doctor', 'nurse', 'surgeon', 'therapist']\n",
      "  Cluster 4: ['teacher', 'tutor', 'instructor', 'tester']\n",
      "\n",
      "Problem 3:\n",
      "  Cluster 0: ['apple', 'banana', 'orange', 'grape']\n",
      "  Cluster 1: ['chocolate', 'vanilla', 'mint', 'carrot']\n",
      "  Cluster 2: ['pizza', 'burger', 'pasta', 'fries']\n",
      "  Cluster 3: ['spinach', 'broccoli', 'lettuce', 'strawberry']\n",
      "  Cluster 4: ['beef', 'chicken', 'pork', 'lamb']\n",
      "\n",
      "Problem 4:\n",
      "  Cluster 0: ['lion', 'tiger', 'cheetah', 'panther']\n",
      "  Cluster 1: ['tulip', 'sunflower', 'daffodil', 'eagle']\n",
      "  Cluster 2: ['shark', 'whale', 'dolphin', 'octopus']\n",
      "  Cluster 3: ['mountain', 'hill', 'valley', 'river']\n",
      "  Cluster 4: ['rose', 'sparrow', 'parrot', 'owl']\n"
     ]
    }
   ],
   "source": [
    "# Run the clustering\n",
    "clustered_words = cluster_words(sample_words_data, num_clusters=5)\n",
    "\n",
    "# Print the results\n",
    "for problem, clusters in clustered_words.items():\n",
    "    print(f\"\\n{problem}:\")\n",
    "    for cluster, words in clusters.items():\n",
    "        print(f\"  Cluster {cluster}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
